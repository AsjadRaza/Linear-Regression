{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e66ce186f96bde03abc8a223e9e9de8",
     "grade": false,
     "grade_id": "cell-6453afbe0bdb6aa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"logo-2020.png\" alt=\"frankfurt school hmi\" style=\"width: 160px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3915b840c68b251334acbf37c7995111",
     "grade": false,
     "grade_id": "cell-acb40d952bd1742d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "___\n",
    "# Machine Learning I.\n",
    "## Problem Set 1: Linear Regression (14 points total)\n",
    "\n",
    "### Instructions\n",
    "The graded portion of problem set 1 consists of one notebook:\n",
    "```\n",
    "ps1_nb1-ml.ipynb\n",
    "```\n",
    "There is also a quick introduction to Jupyter notebooks and a non-graded tutorial of numpy arrays:\n",
    "```\n",
    "0-ten_minute_introduction_to_notebooks.ipynb\n",
    "1-numpy_introduction.ipynb\n",
    "```\n",
    "Both are recommended. \n",
    "\n",
    "### Due Date\n",
    "* Group D 19.02.2020 before 23:59:59 (CET)\n",
    "\n",
    "### Instructor\n",
    "* Prof. Dr. Gregory Wheeler ([gregorywheeler.org](http://gregorywheeler.org))\n",
    "\n",
    "---\n",
    "\n",
    "### Declare your collaborators\n",
    "You may work alone or in a group. The maximum group size is 4 people. \n",
    "\n",
    "If you work in a group, use the next cell to enter the list of names (first, last) of your collaborators. \n",
    "~~~python\n",
    "# Example\n",
    "COLLABORATORS = ['Stu Dent', 'May Bee', 'Ki Val Storr']\n",
    "~~~\n",
    "You should also familiarize yourself with the collaboration policy on the course Canvas page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure your names are strings\n",
    "COLLABORATORS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a45a5aa786fb3bbdaaf9ed66b7d6ae0",
     "grade": false,
     "grade_id": "cell-b62ce31b7ef69ad4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "---\n",
    "## <i>Money cannot buy happiness. Or can it?</i>\n",
    "\n",
    "The populations of some countries are happier than other countries. In this exercise, you will investigate the relationship between a country's per capita GDP and its \"happiness score\".  Since 2012, the annual [World Happiness Report](https://worldhappiness.report/), published by the United Nations' [Sustainable Development Solutions Network](https://www.unsdsn.org/), asks people around the world how happy they are. The happiness score uses the [Cantril scale](https://news.gallup.com/poll/122453/understanding-gallup-uses-cantril-scale.aspx), which is a subjective scale constructed by asking respondents to imagine a ladder on which the best possible life for themselves receives the score of 10, the worst possible life a score of 0, and then are asked to rate their own lives on that scale between 0 and 10.  The questionaire is given to a representative sample of each country each year.\n",
    "\n",
    "We expect more individual wealth to correlate with higher happiness. But, how much of the variation in happiness is explained by individual wealth?  That is the question we will explore.\n",
    "\n",
    "There are three main tasks in this notebook.\n",
    "\n",
    " - PART A. Implement Gradient Descent to calculate the coefficients of a univariate linear regression\n",
    " - PART B. Use a built-in library to check your implementation in A and perform some analysis \n",
    " - PART C. Prepare data yourself and apply your model. \n",
    " \n",
    "You will be asked to write some code and also answer several questions about the model and data. The first step is to run the next cell to load the basic libraries we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "206fae5c66a46f9835a8e9164601debb",
     "grade": false,
     "grade_id": "cell-3683f8c3c14a645b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import numpy and set up plotting parameters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d729fa9ac39acb98c247c56385ae40ea",
     "grade": false,
     "grade_id": "cell-cda1152bfc9e0c0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART A - Gradient Descent\n",
    "\n",
    "## I. Loading and Formatting Data\n",
    "In the ps1 directory you will find the data set for problem 1, `ps1_data.csv`. File loading methods vary depending on the format of the data file. This particular data set is a _comma-seperated text file_ (.csv), so we will use the numpy function `np.loadtxt` and specify that the columns are delimited by commas.\n",
    "\n",
    "Run the next cell to load `ps1_data.csv` into a numpy array assigned to the variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('ps1_data1.csv', delimiter=',')\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.61646318 7.53700018]\n",
      " [1.48238301 7.52199984]\n",
      " [1.48063302 7.50400019]\n",
      " [1.56497955 7.49399996]\n",
      " [1.44357193 7.46899986]\n",
      " [1.50394464 7.37699986]\n",
      " [1.47920442 7.31599999]\n",
      " [1.40570605 7.31400013]\n",
      " [1.49438727 7.28399992]\n",
      " [1.48441493 7.28399992]\n",
      " [1.37538242 7.21299982]\n",
      " [1.10970628 7.079     ]\n",
      " [1.48709726 7.00600004]\n",
      " [1.54625928 6.99300003]\n",
      " [1.53570664 6.97700024]\n",
      " [1.48792338 6.95100021]\n",
      " [1.46378076 6.89099979]\n",
      " [1.7419436  6.86299992]\n",
      " [1.44163394 6.71400023]\n",
      " [1.25278461 6.65199995]\n",
      " [1.62634337 6.64799976]\n",
      " [1.10735321 6.63500023]\n",
      " [1.35268235 6.60900021]\n",
      " [1.18529546 6.59899998]\n",
      " [1.15318382 6.57800007]\n",
      " [1.69227767 6.57200003]\n",
      " [1.34327984 6.52699995]\n",
      " [1.2175597  6.454     ]\n",
      " [0.87200195 6.454     ]\n",
      " [1.23374844 6.45200014]\n",
      " [1.43092346 6.44199991]\n",
      " [1.12786877 6.42399979]\n",
      " [1.43362653 6.42199993]\n",
      " [1.38439786 6.40299988]\n",
      " [1.87076569 6.375     ]\n",
      " [1.07062232 6.35699987]\n",
      " [1.53062356 6.34399986]\n",
      " [1.3613559  6.16800022]\n",
      " [1.63295245 6.10500002]\n",
      " [1.32539356 6.09800005]\n",
      " [1.48841226 6.08699989]\n",
      " [1.29121542 6.08400011]\n",
      " [0.7372992  6.0710001 ]\n",
      " [1.0008204  6.0079999 ]\n",
      " [0.9097845  6.00299978]\n",
      " [1.29178786 5.97300005]\n",
      " [0.78644109 5.9710002 ]\n",
      " [1.39506662 5.96400023]\n",
      " [1.2817781  5.96299982]\n",
      " [0.90797532 5.95599985]\n",
      " [1.41691518 5.92000008]\n",
      " [1.31458235 5.90199995]\n",
      " [1.09186447 5.87200022]\n",
      " [1.26074862 5.8499999 ]\n",
      " [1.40167844 5.83799982]\n",
      " [0.72887063 5.83799982]\n",
      " [1.21768391 5.82499981]\n",
      " [0.83375657 5.82299995]\n",
      " [1.13077676 5.82200003]\n",
      " [1.28455627 5.81899977]\n",
      " [1.34691131 5.80999994]\n",
      " [1.34120595 5.7579999 ]\n",
      " [1.03522527 5.71500015]\n",
      " [1.18939555 5.62900019]\n",
      " [1.35593808 5.62099981]\n",
      " [1.32087934 5.61100006]\n",
      " [1.15655756 5.56899977]\n",
      " [1.10180306 5.5250001 ]\n",
      " [1.19827437 5.5       ]\n",
      " [0.93253732 5.49300003]\n",
      " [1.55167484 5.47200012]\n",
      " [0.85769922 5.42999983]\n",
      " [1.06931758 5.39499998]\n",
      " [0.99101239 5.33599997]\n",
      " [1.28601193 5.32399988]\n",
      " [0.92557931 5.31099987]\n",
      " [1.22255623 5.29300022]\n",
      " [0.95148438 5.2789998 ]\n",
      " [1.08116579 5.27299976]\n",
      " [0.72688353 5.26900005]\n",
      " [0.99553859 5.26200008]\n",
      " [1.1284312  5.25      ]\n",
      " [1.12112904 5.23699999]\n",
      " [0.87811458 5.23500013]\n",
      " [1.15360177 5.23400021]\n",
      " [1.07937384 5.23000002]\n",
      " [1.28948748 5.22700024]\n",
      " [1.07498753 5.2249999 ]\n",
      " [1.31517529 5.19500017]\n",
      " [0.98240942 5.18200016]\n",
      " [0.73057312 5.18100023]\n",
      " [1.06457794 5.17500019]\n",
      " [0.02264318 5.15100002]\n",
      " [0.78854757 5.07399988]\n",
      " [0.78375626 5.07399988]\n",
      " [0.52471364 5.04099989]\n",
      " [0.88541639 5.01100016]\n",
      " [0.59622008 5.00400019]\n",
      " [0.47982019 4.96199989]\n",
      " [1.02723587 4.95499992]\n",
      " [1.05469871 4.829     ]\n",
      " [1.00726581 4.80499983]\n",
      " [0.71624923 4.7750001 ]\n",
      " [0.98970181 4.73500013]\n",
      " [1.16145909 4.71400023]\n",
      " [0.36842093 4.70900011]\n",
      " [0.56430536 4.69500017]\n",
      " [1.15687311 4.69199991]\n",
      " [0.99619275 4.64400005]\n",
      " [0.58668297 4.6079998 ]\n",
      " [0.96443433 4.57399988]\n",
      " [0.56047946 4.55299997]\n",
      " [0.23430565 4.55000019]\n",
      " [0.36711055 4.54500008]\n",
      " [0.47930902 4.53499985]\n",
      " [0.63640678 4.51399994]\n",
      " [1.10271049 4.49700022]\n",
      " [1.19821024 4.46500015]\n",
      " [0.33923384 4.46000004]\n",
      " [1.00985014 4.44000006]\n",
      " [0.90059674 4.37599993]\n",
      " [0.79222125 4.31500006]\n",
      " [0.64845729 4.29199982]\n",
      " [0.80896425 4.29099989]\n",
      " [0.95061266 4.28599977]\n",
      " [0.09210235 4.28000021]\n",
      " [0.47618049 4.19000006]\n",
      " [0.60304892 4.17999983]\n",
      " [0.6017651  4.16800022]\n",
      " [0.65951669 4.13899994]\n",
      " [0.66722482 4.11999989]\n",
      " [0.89465195 4.0960002 ]\n",
      " [0.38143071 4.08099985]\n",
      " [0.35022771 4.03200007]\n",
      " [0.16192533 4.02799988]\n",
      " [0.23344204 3.97000003]\n",
      " [0.43801299 3.93600011]\n",
      " [0.37584653 3.875     ]\n",
      " [0.52102125 3.80800009]\n",
      " [0.85842818 3.79500008]\n",
      " [0.40147722 3.79399991]\n",
      " [1.12209415 3.76600003]\n",
      " [0.43108541 3.65700007]\n",
      " [0.30580869 3.64400005]\n",
      " [0.36861026 3.60299993]\n",
      " [0.59168345 3.59299993]\n",
      " [0.39724863 3.59100008]\n",
      " [0.11904179 3.53299999]\n",
      " [0.24454993 3.50699997]\n",
      " [0.30544472 3.49499989]\n",
      " [0.36874589 3.47099996]\n",
      " [0.77715313 3.46199989]\n",
      " [0.51113588 3.34899998]\n",
      " [0.09162257 2.90499997]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.size(data,0)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3403e8f373aa43b72afa29a1ea38569f",
     "grade": false,
     "grade_id": "cell-3feec8c3b2a19f6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall that numpy arrays are Python's standard datatype for computing with vectors, matrices, and tensors, and the numerical methods of numpy -- such as `np.dot` and `np.sum` -- are element-wise operations. Although there is also a Python matrix package designed specifically to do linear algebra operations, we will use numpy.\n",
    "\n",
    "(For those of you new to Python but who are familiar with linear algebra or with programming matrix operations in MATLAB, [this page](https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/matrix-multiplication-intro) contains useful comparison between Python and MATLAB to help you get quickly up to speed.)\n",
    "\n",
    "You can confirm that data is a numpy array by checking its type, i.e. by excuting `type(data)`. It is also important to know about the dimensions of `data` -- that is, the number of rows (training examples) and the number of columns (features and target). This information is given by using the method `.shape`, i.e., by executing `data.shape`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> You can insert a cell anywhere (+) and delete it (scissors) or any other cell that is not locked\n",
    "    \n",
    "<img src=\"ps_fig1.png\" alt=\"ps3_fig1\" style=\"width: 100px;\"/>  \n",
    "\n",
    "However, before turning in your assignment, you should remove any cells you have added.  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To clear the output of your notebook and your local memory, you can restart the kernel and clear  â€œKernel >> Restart and Clear all Output\":\n",
    "\n",
    "<img src=\"ps_fig2.png\" alt=\"ps3_fig2\" style=\"width: 250px;\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ed4d1a1d733eeb9d0f02986d9efa0fe",
     "grade": false,
     "grade_id": "cell-61e93e897ed5317a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should see that `data` has two columns. The first column is the feature $x$, <b>per Capita GDP</b> of each country in `data`, and the second column is the target $y$, the corresponding <b>Happiness index</b>. The number of rows denotes the number of training examples, which by convention is assigned to the variable `m`.\n",
    "\n",
    "In the next cell, you should complete the function `set_variables()` which will take an array as an argument and return three variables: `x` and `y` , each of which is itself an array; and `m`, an integer. When `set_variables()` is called specifically on `data`, the three variables are\n",
    "\n",
    "    x - the features of psl_data; i.e., the first column, which are city populations\n",
    "    y - the target variable of psl_data; i.e., the second column, which are revenues\n",
    "    m - the number of training examples in psl_data.\n",
    "\n",
    "<u>Your code should work on <b>any</b> two-column array</u>, not only on `data`.\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Tip</b>: Remember that arrays in Python are 0-indexed.</div>\n",
    "\n",
    "\n",
    "\n",
    "### Completing Functions\n",
    "\n",
    "The next cell has a partially completed function, `set_variables()`.  Inside the code block, you will see two lines:\n",
    "\n",
    "~~~python\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "~~~\n",
    "\n",
    "To complete the function, you should delete both of these lines and replace the comment `# YOUR CODE HERE` with your line or lines of code.  The second line raises a  \"not implemented error\" and is included automatically to guard against accidentally submitting an incomplete assignment.  To pass the final validation, you must remove each of these errors from your functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0b07ef437c33001dd39d42ffbac8ff2",
     "grade": false,
     "grade_id": "setVariables",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def setVariables(data_array) :\n",
    "    \"\"\"\n",
    "    The function set_variables() is called on one argument:\n",
    "    \n",
    "        data_array - an np.array \n",
    "        \n",
    "    and returns three global variables:\n",
    "        x - the first column of data_array\n",
    "        y - the second column of the data_array\n",
    "        m - the number of rows of the data_array na\n",
    "    Your code should run on any two-column data_array.\n",
    "    \"\"\"\n",
    "    global x\n",
    "    global y\n",
    "    global m\n",
    "    # YOUR CODE HERE\n",
    "    x = data_array[:,0]\n",
    "    y = data_array[:,1]\n",
    "    m = np.size(data_array,0)\n",
    "    #raise NotImplementedError()\n",
    "    return x, y, m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b94a24af6669307c652b6a6d4e53fb9d",
     "grade": false,
     "grade_id": "cell-6a1a3dea65d20fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Test Cells\n",
    "The next three cells are test cells that will evaluate your implementation of `setVariables()`. You should run all three. (They will be run automatically when your notebook is graded.) Each of these cells has a line of code, which is a \"public\" test of your code, but there are also \"hidden\" tests of your code.  The public tests are designed to give you some initial feedback that you are on the right track.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e68614f6e7d05c3d02efb5f852fe063",
     "grade": true,
     "grade_id": "setVariables-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TEST CELL 1 for set_variables\"\"\"\n",
    "setVariables(data)\n",
    "assert x[34] == 1.870765686 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e69c74b0d093494cb379b3d2b82e4f09",
     "grade": true,
     "grade_id": "setVariable-test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TEST CELL 2 for set_variables\"\"\"\n",
    "setVariables(data)\n",
    "assert y[71] == 5.429999828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd3b769e852e1a10ea1f9f829c098984",
     "grade": true,
     "grade_id": "setVariable-test3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"TEST CELL 3 for m of set_variables\"\"\"\n",
    "setVariables(data)\n",
    "assert m < 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3802d09ac28c5cd087e01c7e3cc077dd",
     "grade": false,
     "grade_id": "cell-537ebf2ac6ef5737",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## II. Data Visualization \n",
    "The following block of code produces a basic scatterplot with a legend and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2170d86828>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# produce scatterplot\n",
    "\n",
    "plt.scatter(x,y, label=\"ps1_data1\")\n",
    "plt.title('2017 World Happiness by Country')\n",
    "plt.xlabel(r'Per Capita GPD')\n",
    "plt.ylabel(r'Happiness Score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1869da2a358ac0e036fee3c2310906e6",
     "grade": false,
     "grade_id": "cell-ccc4359102f38e22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## III. Implementing Gradient Descent\n",
    "\n",
    "\n",
    "Now we will implement gradient descent to fit the **linear regression** parameters $\\begin{align} \\theta & = (\\theta_0,\\theta_1)\\end{align}$ to the data set, `data`.\n",
    "\n",
    "### Univariate Linear Regression\n",
    "\n",
    "A univariate linear regression has one _independent variable or regressor, x,_ which is called a _feature_ in machine learning. The _dependent variable_ or _target_ variable $y$, is the  true happiness score.  The goal is to fit a linear model to estimate or predict the value of $y$, happiness, as some linear function of $x$, per capita GDP. Specifically, the hypothesis $\\begin{align} h(x; \\theta) \\end{align}$ is determined by $\\begin{align} \\boldsymbol{\\theta} \\end{align}$, a two-dimensionpal vector $\\theta = (\\theta_0,\\theta_1)$, and the single feature $ x_1$, such that $$h(x; \\theta) = \\theta_0 + \\theta_1x_1.$$\n",
    "\n",
    "\n",
    "To fit a model of this form to data, we need a way to pick values for $\\theta_0$ and $ \\theta_1$ that \"best fit\" the data. Intuitively, we want to select parameters that minimize the error of its estimates of the known values for $y$ in the training data. Specifying, we will use <b>residual squared error (RSS)</b> as the numeric measure of performance and <b>ordinary least squares</b> as the definition of \"best fit.\"  Defining the precise form of this minimization task is done by defining the optimization objective of the algorithm, which is the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22db03945e429730790c5f0a7be2fe27",
     "grade": false,
     "grade_id": "cell-daf616dda72c96f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Optimization Objective\n",
    "\n",
    "The optimization objective of linear regression is to minimize the <b>residual sum of squares</b>, RSS,  \n",
    "\n",
    "$$RSS = \\sum_{i=1}^m (\\hat{\\epsilon}^{(i)})^2 = \\sum_{i=1}^{m} (y^{(i)} - (\\hat{\\theta}_0 + \\hat{\\theta}_1x^{(i)} ))^2$$\n",
    "\n",
    "where RSS is the standard <b>loss function</b> used for fitting linear regression models.  The <b>mean squared error</b> (MSE) is $\\frac{RSS}{m}$.  The cost function $J(\\boldsymbol{\\theta})$ we will minimize is\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\frac{1}{2m}\\sum_{i=1}^m (h(x^{(i)}; \\boldsymbol{\\theta}) - y^{(i)})^2$$\n",
    "\n",
    "which is $\\frac{1}{2} * MSE$. \n",
    "\n",
    "The index $i$ ranges over the number of training examples, $m$, in `data`. Informally, the cost function $J$ applied to $\\begin{align} \\boldsymbol{\\theta} & = (\\hat{\\theta}_0,\\hat{\\theta}_1)\\end{align}$ assesses the \"cost\" of using particular values for $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ to fit a line to `data`, measured as squared-error loss, and the optimization problem is one of picking values for $\\hat{\\theta}_0$ and for $\\hat{\\theta}_1$ that makes those costs as close to zero as possible. Unpacking the right side of this equation further, for each training example $i$ (that is, for each $i$ of the $m$ rows of `data` ), a hypothesis $h$ parameterized by $\\boldsymbol{\\theta}$ looks at the feature $x$ of the $i$th training example, written $x^{(i)}$, and computes an estimate of the value of $y$ for this $i$th example, written $h(x^{(i)}; \\boldsymbol{\\theta})$. The total loss then is the average squared-error loss for all $m$ training examples.\n",
    "\n",
    "Specifically, the optimization objective is to minimize the cost function $J(\\boldsymbol\\theta)$, written as $\\min_{\\boldsymbol{\\theta}} J(\\boldsymbol\\theta)$. The fraction $\\frac{1}{2}$ and changing $(y - h)^2$ in RSS to $(h- y)^2$ are each mathematical conveniences that make the next step simplier and more intuitive, respectively. \n",
    "\n",
    "<div class=\"alert alert-info\"><b>Notation 1</b>: Recall that in frequentist statistics $\\theta$ is a true but unknown parameter and $\\hat{\\theta}$ is an estimate of that unknown parameter. In short, the <b>estimators</b> that we construct, manipulate, and evaluate all wear hats. But, since we never deal directly with \"true but unknown\" parameters, when the discussion turns to coding we drop the hat notation. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3de7cae84d978b572ae04ea9a5b1983",
     "grade": false,
     "grade_id": "cell-0d2eb27e9dd8e26d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "### Implementation\n",
    "\n",
    "The adjustable parameters of a univariate regression model $h(x^{(i)}; \\boldsymbol{\\theta})$  are $\\theta_0$ and $\\theta_1$ -- or, if you did not read the previous remark about notation, $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$. The parameters $\\theta_0$ and $\\theta_1$  determine the $y$-intercept and slope, respectively.  They are the values that gradient descent changes to minimize the cost $J(\\boldsymbol{\\theta})$.\n",
    "\n",
    "There are a family of gradient descent algorithms. We will implement **batch gradient descent**. In batch gradient descent, at each iteration of the algorithm the values for the parameter vector $\\boldsymbol{\\theta}$ (should!) change  closer to the optimal values that realize the lowest cost $ J(\\boldsymbol\\theta)$.  \n",
    "\n",
    "<div class=\"alert alert-info\"><b>Notation 2</b>:  The actual optimal parameter values that minimizes $J$ are unknown.  Were our discussion to dwell on the difference between the actual parameters and our estimated parameters, then we'd be compelled to put hats on the estimated parameters. </div>\n",
    "\n",
    "\n",
    "As we discussed in class, at each step batch gradient descent **simultaneously updates** all parameters $\\begin{align} \\theta_j \\end{align}$: <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $\\begin{align}\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(h(x^{(i)}; \\boldsymbol{\\theta}) - y^{(i)})x^{(i)} \\end{align}$ simultaneous update of $\\begin{align} \\theta_j \\end{align}$ for all _j_.\n",
    "\n",
    "The next series of steps guides you through an implementation of gradient descent.\n",
    "\n",
    "___\n",
    "\n",
    "**Step 1**. Observe that there are two parameters, $\\theta_0$ and $\\theta_1$, but only one feature in `data`. (Check the shape of `x` if you are unconvinced.) Since $\\theta_0$ is the y-intercept term, this is the same as if we multiplied $\\theta_0$ by 1, for each <i>m</i> entries. We say that $x_0$ = 1 is a **intercept term**, and we add intercept terms in such cases to ensure that feature vectors and parameter vectors are <b>comformable</b>.\n",
    "\n",
    "With this explanation in mind, add a column of 1's to the single column feature vector within data. We do this first by initializing an $(m, 2)$ array of 1's, then replacing the second column of <i>X</i> with the first column of `data`. This new row of ones is to accommodate the $y$-intercept parameter, $\\theta_0$. The next step is to\n",
    "initialize a $(1, 2)$ array of zeros, called `theta` , which initializes the values for $\\theta_0$ and $\\theta_1$, respectively. Our gradient descent algorithm will update `theta` in order to minimize the cost function <i>J</i>.\n",
    "\n",
    "Running the next cell initializes the variables `x` and `theta` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ae1fd9d210acb74b6ed2412f8f3b078",
     "grade": false,
     "grade_id": "cell-bbc0ed832ebb3d2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize data array\n",
    "X = np.ones((m, 2))\n",
    "X[:,1] = data[:,0]\n",
    "# initalize model\n",
    "theta = np.zeros(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c28bffa357d4b3aab2135afb6814dd0",
     "grade": false,
     "grade_id": "cell-30394b510bba058a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 1\n",
    "\n",
    "Suppose that in addition to \"<i>per capita GDP</i>\" you also had the average \"<i>life expectancy</i>\" for each country to use as features in a linear model to estimate each country's <i>happiness score</i>. \n",
    "\n",
    "Which of the following statements are true? Select all and only that apply.\n",
    "\n",
    "* A) $h(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$\n",
    "* B) $h(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\theta_0 + \\theta_1x_1 + x_2$\n",
    "* C) $h(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\theta_0 + \\theta_1x_1 x_2$\n",
    "\n",
    "Suppose now you wish to initialize a data array and parameter vector for this model.  Which block of code could you use for this model?  (Assume as above that the last column of `data` is the target, $y$.)\n",
    "\n",
    "* D) \n",
    "~~~python\n",
    "# initialize data array\n",
    "X = np.ones((m, 3))\n",
    "X[:,1] = data[:,0]\n",
    "# initalize model\n",
    "theta = np.zeros(2)\n",
    "~~~\n",
    "\n",
    "* E) \n",
    "~~~python\n",
    "# initialize data array\n",
    "X = np.ones((m, 3))\n",
    "X[:,0] = data[:,0]\n",
    "X[:,1] = data[:,1]\n",
    "# initalize model\n",
    "theta = np.zeros(3)\n",
    "~~~\n",
    "\n",
    "* F) \n",
    "~~~python\n",
    "# initialize data array\n",
    "X = np.ones((m, 3))\n",
    "X[:,1] = data[:,0]\n",
    "X[:,2] = data[:,1]\n",
    "# initalize model\n",
    "theta = np.zeros(3)\n",
    "~~~\n",
    "\n",
    "From the list of possible answers `['A', 'B', 'C', 'D', 'E', 'F]` , record your answer or answers as a <b>list</b> in the next cell by completing the function `ans_one()`.  For example, if the possible answers were `['P','Q','R']` and you selected P and Q as the correct answers, then you would simply write:\n",
    "~~~python\n",
    "ans = ['P', 'Q']\n",
    "~~~\n",
    "in the function `ans_one()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b8ecf4e123a6182ce1dc7bf17555fb4",
     "grade": false,
     "grade_id": "ans_one-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_one():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    ans = ['A', 'F']\n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2744205949f1e7e8e7df3d14ca36c00d",
     "grade": true,
     "grade_id": "ans_one-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for ans_one()\n",
    "ans = ans_one()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "if type(ans) == list and all(ii in possible_ans for ii in ans):\n",
    "    assert True\n",
    "else:\n",
    "    raise AssertionError(\"Inadmissible answer. Check that your answer to ans_one is the correct format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfe04d786ee6a9e67939e063ea0561a9",
     "grade": false,
     "grade_id": "cell-98e89d3a79671865",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Step 2.** Compute the squared-error cost function from above:\n",
    "\\begin{align} \\frac{1}{2m}\\sum_{i=1}^m (h(x^{(i)}; \\boldsymbol{\\theta}) - y^{(i)})^2 \\end{align}\n",
    "\n",
    "The function `costJ(X, y, theta)` takes as arguments the (_m_, 2)-dimension features array, `X` ,the (_m_, 1)-dimension target column vector, `y`, and the initial two-element parameter vector `theta`.\n",
    "\n",
    "There are several ways to implement this code, but a good strategy is to break the operation down into several steps and implement those steps as separate lines of code. Here is one way to attack the problem in two lines of code.\n",
    "\n",
    "<u>First Line of Code</u>\n",
    "\n",
    "Setting aside the indices, notice the general form of the hypothesis: $$h(x; \\boldsymbol{\\theta}) = \\theta_0x_0 + \\theta_1x_1.$$ Our first operation, then, is to multiply each $x^{(i)}$ in the second column of X by $\\theta_1$ and to keep $\\theta_0$ as is, which is why we added a column of 1s to X above: that is, $\\begin{align} x_0 =  1\\end{align}$ for all _m_ training examples. Then, for each estimate you can subtract the corresponding values of $y$.  \n",
    "\n",
    "<div class=\"alert alert-info\"> <b>Hint</b>: We deliberately highlighted the dimensions of the arrays 'X' and â€˜thetaâ€™ because there is a nice result from matrix algebra. Let $\\boldsymbol{X}$ and $\\boldsymbol{\\theta}$ be:\n",
    "    \n",
    " $$\\boldsymbol{X} = \\begin{bmatrix} \n",
    " \t\t\t\t1 & x_1^{(1)}  \\\\ \n",
    "\t\t\t\t1 & x_1^{(2)}   \\\\ \n",
    "\t\t\t\t1 & x_1^{(3)}   \\\\ \n",
    "\t\t\t\t\\vdots & \\vdots \\\\\n",
    "\t\t\t\t1 & x_1^{(m)}  \n",
    "\t\t\t\\end{bmatrix}\n",
    "\t\t\t%\n",
    "\t\t\t\\quad\n",
    "\t\t\t%\n",
    "\t\t\t%\n",
    " \\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1\n",
    "\t\\end{bmatrix}$$\n",
    "    \n",
    "Then,    \n",
    "$$\\begin{bmatrix} \n",
    " \t\t\t\t1 \\\\\n",
    "\t\t\t\t1 \\\\\n",
    "\t\t\t\t\\vdots \\\\\n",
    "\t\t\t\t1\n",
    "\t\t   \\end{bmatrix}\n",
    "\\theta_0  +\n",
    "%\n",
    " \\begin{bmatrix} \n",
    " \t\t\t\tx_1^{(1)}  \\\\\n",
    "\t\t\t\tx_1^{(2)}  \\\\\n",
    "\t\t\t\t\\vdots \\\\\n",
    "\t\t\t\tx_1^{(m)} \n",
    "\t\t   \\end{bmatrix}\n",
    "\\theta_1\n",
    "= \n",
    "\\begin{bmatrix} \n",
    " \t\t\t\t\\theta_0 + \\theta_1x^{(1)} \\\\ \n",
    "\t\t\t\t\\theta_0 + \\theta_1x^{(2)} \\\\ \n",
    "\t\t\t\t\\vdots \\\\\n",
    "\t\t\t\t\\theta_0 + \\theta_1x^{(m)} \\\\ \n",
    "\t\t\t\\end{bmatrix} = \\boldsymbol{X\\theta}$$\n",
    "\n",
    "where $\\boldsymbol{X\\theta}$ is an $m \\times 1$ vector. \n",
    "\n",
    "See the tutorial, `ps1-tutorial.ipynb`, for information about `np.dot` specifically, and about matrix multiplication in general.</div>\n",
    "\n",
    "The first step is to compute the difference between each of the $m$ estimates in the vector $\\boldsymbol{X\\theta}$ and the corresponding target values in the vector $y$. The next step is to square this difference, which we may use the built-in NumPy function `np.power(---,---)`.\n",
    "\n",
    "So, the vector of quantities $(h(x; \\boldsymbol{\\theta}) - y^{(i)})^2$ for each _i_ of the _m_ examples can be computed and stored to a local variable `scores` with one line of code of the form\n",
    "\n",
    "~~~python\n",
    "# YOUR CODE HERE\n",
    "scores = np.power(---, ---)\n",
    "return\n",
    "~~~  \n",
    "\n",
    "\n",
    "<u>Second Line of Code</u>\n",
    "\n",
    "What remains is to sum `scores` and divide by the number of training examples multiplied by two. The result should be a scalar saved to the variable `ans`:\n",
    "\n",
    "~~~python\n",
    "# YOUR CODE HERE\n",
    "scores = np.power(---, ---)\n",
    "ans = ---\n",
    "return ans\n",
    "~~~\n",
    "\n",
    "where `ans` will use the value computed in the first line of code and saved to the variable `scores`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "979c88f3536c3cc985b98515affebcec",
     "grade": false,
     "grade_id": "costJ",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def costJ(X, y, theta):\n",
    "    \"\"\"Implement the squared-error cost function, compute_cost_J.\n",
    "    compute_cost_J(X, y, theta) computes the cost of using the\n",
    "    theta to parameterize a linear regression hypothesis to fit\n",
    "    the data in X and y.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    m = len(y)\n",
    "    prediction  = np.dot(X, theta)\n",
    "    difference  = prediction - y\n",
    "    scores = np.power(difference, 2)\n",
    "    ans = np.sum(scores / (2 * m)) \n",
    "    #raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bb4f2067146165e30330da64d386fa9",
     "grade": true,
     "grade_id": "costJ-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test cell 1 for costJ\"\"\"\n",
    "assert round(costJ(X,y,theta), 1) == 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ded5137b7a4a0c66af4628d4dce2ec54",
     "grade": true,
     "grade_id": "costJ-test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test cell 2 for costJ'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Test cell 2 for costJ\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "881a9da468b77b7ba18c4b9b60379f3f",
     "grade": false,
     "grade_id": "cell-4d71ef4a29312e25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Step 3**. We now turn to the implementation of gradient descent. The function,\n",
    "    \n",
    "    batchGradientDescent(X, y, theta, alpha, num_iterations)\n",
    "    \n",
    "takes 5 arguments:\n",
    "\n",
    "    X - an array of features from data\n",
    "    y - an array of labels from data\n",
    "    theta - an array of parameters that gradient descent adjust to minimize loss according to J_cost\n",
    "    alpha - the learning rate\n",
    "    num_iterations - the number of iterations that the algorithm runs before terminating.\n",
    "\n",
    "The loop structure to iterate gradient descent `num_iterations` times is given for you. What is missing are the simultaneous updates of the two components of __$\\theta$__, namely $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "\n",
    "\n",
    "__Gradient Descent Algorithm__\n",
    "\n",
    "    Enter loop\n",
    "        Repeat:\n",
    "               \n",
    "$$\\theta_0=:\\theta_0 - \\alpha \\frac{\\partial} {\\partial \\theta_0} J(\\theta_0, \\theta_1)$$\n",
    "$$\\theta_1=:\\theta_1 - \\alpha \\frac{\\partial} {\\partial \\theta_1} J(\\theta_0, \\theta_1)$$\n",
    "         \n",
    "        Stop at convergence\n",
    "    Exit loop\n",
    "    \n",
    "\n",
    "What you will need to do complete two steps within the for loop, namely update theta and create a history of the computed cost function _J($\\theta$)_ for each\n",
    "iteration of the algorithm.\n",
    "\n",
    "\n",
    "__1. Update theta__\n",
    "\n",
    "The code initializes `theta_new` as a copy of theta. Within the for loop, your simultaeous update of `theta_new` should be completed in one line of\n",
    "_vectorized code_:\n",
    "\n",
    "```python \n",
    "--- -= --- <operation> np.dot(---, ---) <operation> --- \n",
    "```\n",
    "            \n",
    "which may involve the following additional pieces, not necessarily in order,  may be used more than once:\n",
    "\n",
    "  * \\`X`\n",
    "  * \\`y`\n",
    "  * \\`theta_new`\n",
    "  * \\`X.T`, the transposition of X\n",
    "  * \\`alpha`\n",
    "  * \\`np.dot(---,---) (operation) ----`\n",
    "  * with \\`(operation)` from the set {â€”, *, /}\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>Hint</b>: The partial derivative terms for each component $\\theta_j$ of the parameter vector $\\boldsymbol{\\theta}$ for is:\n",
    "$$\n",
    "\\theta_j:   {\\frac{\\partial}{\\partial\\theta_j}  J(\\theta_1, \\ldots, \\theta_j \\ldots, \\theta_n)} =  \\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)}; \\boldsymbol{\\theta}) - y^{(i)}) \\cdot x_j^{(i)}, $$\n",
    "where $x_j^{(i)} = 1$ for all $i$. \n",
    "    \n",
    "We saw above that $h(x^{(i)}; \\boldsymbol{\\theta}) - y^{(i)})$ is calculated by $X \\theta$. Then, the partial derivatives are either computed by $(X \\theta) X$ or $(X \\theta) X^T$. Which is it? Why? </div>\n",
    "  \n",
    "__2. Store history of the cost function J__\n",
    "\n",
    "The second step is to write into the $j$th element of the vector `J_history` the result from computing the cost determined by executing costJ using `theta_new` that was just updated in the $j$th iternation. Bear in mind that `ii` is used as the index for `num_iterations`, to avoid confusing this for indices used to index rows ($i$) and columns ($j$) of our training set.\n",
    "___\n",
    "\n",
    "Now, complete the implementation of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "421667b45c33b5ea5e9d5511dce564a8",
     "grade": false,
     "grade_id": "cell-c89997583053c8e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def batchGradientDescent(X, y, theta, alpha, num_iterations):\n",
    "    \"\"\"gradient_descent performs batch gradient descent to minimize theta and return a history\n",
    "    of the gradient steps to plot.\n",
    "\n",
    "    gradient_descent updates theta by taking a number of gradient steps, fixed by the\n",
    "    parameter num_iterations, where the size of those steps is determined by the learning\n",
    "\n",
    "    rate, alpha.\n",
    "    \"\"\"\n",
    "    ## Initialize variables\n",
    "    # m is the number of training samples\n",
    "    m= len(y)\n",
    "\n",
    "    # J_history is the cost function history, intialized to a vector of zeros\n",
    "    # for each iteration step\n",
    "    J_history = np.zeros(num_iterations)\n",
    "    # initialize theta_new with a copy of theta.\n",
    "\n",
    "    theta_new = theta.copy()\n",
    "    for ii in range(num_iterations):\n",
    "        # 1st line: update theta_new\n",
    "        # 2nd line: update J_history with compute_cost using theta_new\n",
    "        # YOUR CODE HERE\n",
    "        predictions = np.dot(X,theta_new)\n",
    "        theta_new = theta_new - (1 / m) * alpha * (X.T.dot(predictions - y))\n",
    "        J_history[ii] = costJ(X, y, theta_new)\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    return theta_new, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06eebdea8ce9b235aeb972b039841a11",
     "grade": false,
     "grade_id": "cell-d15c4c2300cad48a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## III. Running Gradient Descent\n",
    "\n",
    "The next step is to set the learning rate and number of iterations. Setting the correct values for these two parameters depends on the data set. In practise, you would need to try out different values of these parameters to see how your alhorithm performed. But,I provide you with parameter settings that will work if your code up to now is implemented correctly.  \n",
    "\n",
    "Specifically, be sure the next cell has the following parameter values:\n",
    "\n",
    "~~~python\n",
    "## gradient_descent adjustable parameters\n",
    "# learning rate\n",
    "alpha = 0.2\n",
    "#number of iterations\n",
    "iterations = 1100\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gradient_descent adjustable parameters\n",
    "# learning rate\n",
    "alpha = 0.2\n",
    "#number of iterations\n",
    "iterations = 1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69accbf7d10fea078af35e666a647dde",
     "grade": false,
     "grade_id": "cell-0b798470b6f642c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the next cell to instantiate `alpha` and `iterations`.\n",
    "Now we are ready to run gradient descent with the parameter settings for `alpha` and the number of `iterations` excuting the following line of code in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent\n",
    "theta_new, J_history = batchGradientDescent(X, y, theta, alpha, iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "acf0a060e6117dceb606b182635400a4",
     "grade": false,
     "grade_id": "cell-0f71100dcf57f657",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Gradient descent (correctly parameterized by `alpha` and `iterations`) returns values for $\\theta_0$ and $\\theta_1$ stored in `theta_new`,along with `J_history`.\n",
    "\n",
    "Run the next cell to see the values for $\\theta_0$ and $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.22558776, 2.164952  ])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the output of batchGradientDescent\n",
    "theta_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df2cc9c121cf7cd772f161063f47fb60",
     "grade": false,
     "grade_id": "cell-ef954a0cd14dbbf5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When implemented correctly, running `batchGradientDescent` on `data` with learning rate `alpha = 0.2` and `iterations = 1300` should return a value for $\\theta_0$ that is aproximately $3.20$ and a value for $\\theta_1$ that is aproximately $2.18$, and `J_history` should be an array of monotonically decreasing values.\n",
    "\n",
    "\n",
    "Run the following test cells to check. Note that there are hidden test cells that will be used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9794e8e1f4254216aaa4f428146048f4",
     "grade": true,
     "grade_id": "BGD-test1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12eb336ff8b4490a14a2e4a8981fc922",
     "grade": true,
     "grade_id": "BDG-test2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3368ce692907c40c2264c994d637e39",
     "grade": true,
     "grade_id": "BGD-test3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bd1dfd2461636987664dba5eb752558",
     "grade": false,
     "grade_id": "cell-9171f4c3368f3043",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## IV. Visualization, Tuning, and Debugging\n",
    "\n",
    "### Plotting the history of J\n",
    "\n",
    "The variable J_history stores the computed cost function for each iteration of batchGradientDescent , which for our example we set to 1100 iterations. How do we determine that number of iterations is long enough to converge? One way to do this is to plot the value of $J(\\theta)$ at each iteration to see its\n",
    "performance.\n",
    "\n",
    "The next cell plots `J_history` with your initial parameters.  If your implementation is correct, your cost function should decrease with each iteration and loosely resemble the following plot:   \n",
    "\n",
    "<img src=\"ps1-fig1.png\" alt=\"sample learning rate\" style=\"width: 400px;\"/>\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Hint:</b> You can use this plot to check whether you have implemented `batchGradientDescent` correctly, and perhaps to give you clues about how to correct\n",
    "your code if you find something amiss.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'$J(\\\\theta)$')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot cost function vs. iteration\n",
    "plt.plot(J_history)\n",
    "plt.xlabel(r'Iteration number' )\n",
    "plt.ylabel(r'$J(\\theta)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f86d6c82d8b21cdcf2b0b82cc418071d",
     "grade": false,
     "grade_id": "cell-5fa3a70db0fd500e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Selecting Learning Rates\n",
    "\n",
    "We selected the learning rate `alpha = 0.2` and `num_iterations = 1100`. The previous plot should give you clues about how to improve the model.  What should you do? You can adjust either the learning rate `alpha` or the number of iterations `iters`. \n",
    "\n",
    "Let's look at the case where `iters = 20` and ask whether a different learning rate would be better in terms of minimizing $J(\\theta)$ within the first 20 iterations. To look at a several learning rates at once, we can define a list\n",
    "\n",
    "~~~python\n",
    "alphas = [ ---, ---, ---, ---, ---]\n",
    "iters = 20\n",
    "~~~\n",
    "\n",
    "to compute and plot different learning rates at once.\n",
    "\n",
    "In the next cell, supply a range of learning rates, to plot and compare with respect to a fixed number of iterations. Note that in this plot we only look at the first 20 iterations.\n",
    "\n",
    "Run the next cell, then answer the Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0, 0, 0, 0, 0]\n",
    "iters = 20\n",
    "for aa in alphas:\n",
    "    theta_new, J_history = batchGradientDescent(X, y, theta, aa, iters)\n",
    "    plt.plot(J_history, label='learning rate={0}'.format(aa) )\n",
    "\n",
    "plt.ylabel(r'$J(\\theta)$')\n",
    "plt.xlabel(r'Number of Iterations' )\n",
    "plt.legend(loc='best', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6bf9aa7513ee5ee14ba38a1f65eab00",
     "grade": false,
     "grade_id": "cell-fb0a79d046b5e584",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 2\n",
    "\n",
    "What is the learning rate `alpha` that yields the lowest score $J(\\theta)$ after exactly 20 iterations? Limit your search to values of `alpha` to two decimal places.  For example, if `alpha = 0.07` is your answer, you would finish function `ans_two()` by writing\n",
    "\n",
    "~~~python\n",
    "num = 0.07\n",
    "~~~\n",
    "\n",
    "Enter your answer below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ae3517551bf7c3c64a3ebb143f7f06d",
     "grade": false,
     "grade_id": "ans_two",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_two():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4872ce5af3eeb4a89692d6110788ca4",
     "grade": true,
     "grade_id": "cell-74cfd101ab4b180f",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num = ans_two()\n",
    "if type(num) == float and num == round(num,2):\n",
    "    assert True\n",
    "else:\n",
    "    raise AssertionError(\"Check that your answer is a float and rounded to two decimal places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b95f2850eecf139cd379ff79e2581d34",
     "grade": true,
     "grade_id": "cell-b8fa64c75b3c251c",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91b465e8254c763f95f8829c0e49d7e0",
     "grade": true,
     "grade_id": "ans_two-test1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce8ec0953069ac0e60ffd6eb0286f264",
     "grade": false,
     "grade_id": "cell-a110ae97236a41a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 3\n",
    "\n",
    "Which are true? Mark all and only that apply.\n",
    "\n",
    "Suppose that $J(\\boldsymbol{\\theta})$ is a continuously differentiable and convex function and $\\theta$ is in $\\mathbb{R}^2$.\n",
    "\n",
    "  * A) Gradient Descent is guaranteed to converge to a local minima of <em>h</em> for all learning rates <em>Î±</em>.\n",
    "  * B) Gradient Descent is guaranteed to converge to a local minima of <em>h</em> for some learning rates <em>Î±</em>\n",
    "  * C) There is no learning rate <em>Î±</em> such that Gradient Descent is guaranteed to converge to a local minima of <em>h</em>\n",
    "  *  D) The gradient of <em>h</em> is zero at the minimum.\n",
    "  * E) The gradient of <em>J</em> points in the direction in $\\mathbb{R}^2$ of the steepest increase in the value of <em>J</em>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75a2021f0ad40a209b44d4767295039",
     "grade": false,
     "grade_id": "ans_three",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_three():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    " \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b928220d6781a1e5b860e37fbaf69f0a",
     "grade": true,
     "grade_id": "ans_three-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for ans_three()\n",
    "ans = ans_three()\n",
    "possible_ans = ['A', 'B', 'C', 'D', 'E']\n",
    "if type(ans) == list and any(ii in ans for ii in possible_ans):\n",
    "    assert True\n",
    "else:\n",
    "    raise AssertionError(\"Inadmissible answer. Check that your answer to ans_one is the correct format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f1e9f19e4acaedd0e53c36995802967",
     "grade": false,
     "grade_id": "cell-1f23e9f991d222b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 4\n",
    "\n",
    "Instead of Gradient Descent, which yields an approximate solution to OLS regression, we can instead use the Normal Equation to calculate an analytical solution. \n",
    "\n",
    "Which of the following are true about the Normal Equation? Mark all and only that apply.\n",
    "\n",
    "  * A) The number of features must be less than the number of training examples.\n",
    "  * B) There is no learning rate hyperparameter to choose.\n",
    "  * C) There is no need to iterate.\n",
    "  * D) The speed of computing the normal equation slows exponentially as the number of features increases\n",
    "  * E) The speed of computing the normal equation slows exponentially as the number of training examples increases\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9937c9179e5cee78d01dce19a64f05f",
     "grade": false,
     "grade_id": "ans_four",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_four():\n",
    "    \"\"\" Returns a list of your answers.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :ans:  list\n",
    "        The list of your answers. Elements of :ans: are strings.\n",
    "    Returns\n",
    "    -------\n",
    "    :ans:\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "553f5556618e30ded83f7d4c16680734",
     "grade": true,
     "grade_id": "ans_four-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1316901f0e8b35646eb7ea47c7b9d8f",
     "grade": false,
     "grade_id": "cell-0804cea1022eff24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART B - SciKit-Learn Linear Regression\n",
    "\n",
    "Naturally, there are built-in libraries for regression models.  In fact, it just takes a few lines of code once we import `LinearRegression` from the `linear_model` methods in the `sklearn` (scikit-learn) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37ff5bdf8e64ee3738bd3e1cdbd914b7",
     "grade": false,
     "grade_id": "cell-efc7c77c6494db74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import sklearn linear regression \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efc0886dd9e4690162488fbffa2e82ac",
     "grade": false,
     "grade_id": "cell-2936014478ce7c0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Sklearn uses a common [API](https://en.wikipedia.org/wiki/Application_programming_interface) for building models. So, the sequence we follow to build a simple linear model resembles the sequence of steps for building a wide range of models.  It is good, therefore, to become aquainted with this tool.\n",
    "\n",
    "Basically, there are two steps:\n",
    "\n",
    " - Instantiate a model\n",
    "  \n",
    " - Fit the model\n",
    "\n",
    "Followed by plotting and some rudimentary anlysis of the model. Later in the course we will add a third step, which involves making predictions and evaluating the accuracy of those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = LinearRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcf12682a14d1ce50cbeb128201723ce",
     "grade": false,
     "grade_id": "cell-018717d26ed6eb9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And that's it! \n",
    "\n",
    "How does this method relate to what you did in PART A?  To retrieve the parameters, we apply the following methods to `model`:\n",
    "\n",
    " - `.intercept_`\n",
    " - `.coef_`\n",
    " \n",
    "Note that the trailing underscore `_` is simply used to avoid naming conflicts in sklearn. You can interpret each as giving you the intercept ($\\theta_0$) and the remaining coefficients ($\\theta_1, \\theta_2, \\ldots$) in your model, respectively. Here (hopefully!) some of what you learned in PART A can be useful to understand what the next lines of code are doing. \n",
    "\n",
    "Specifically, since sklearn linear regression can model multiple regression problems (i.e., problems with 2 or more features), the method `.coef_` returns an np.array rather than single number.  Hence, to retrieve the parameter value for $\\theta_1$, we index by `[1]` into this array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print theta0 and theta1\n",
    "print('theta_0:', model.intercept_)\n",
    "print('theta_1:', model.coef_[1])  # model.coef_ returns an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f6afe111fe1cc6fb453764d0bd6a187",
     "grade": false,
     "grade_id": "cell-3344f2765f1b6622",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compare the values you computed here to those you computed above.  \n",
    "\n",
    "Next, we produce a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.title('2017 World Happiness by Country')\n",
    "plt.xlabel(r'Per Capita GPD')\n",
    "plt.ylabel(r'Happiness Score')\n",
    "plt.plot(x, model.predict(X),color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea0a77ce6d44b1af2d50a91771e74ca4",
     "grade": false,
     "grade_id": "cell-8b0a718a767976dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The <b>coefficient of determination</b> or \"r-squared\", $r^2$, is a measure of how well a linear regression model fits our data.  Put more carefully, $r^2$ measures how much of the variation in the data is explained by the (best fit) model $h(x; \\theta)$:\n",
    "\n",
    "$$ r^2 := 1- \\frac{SE \\, \\hat{y}}{SE \\, \\overline{y}} $$\n",
    "\n",
    "where $SE \\,\\hat{y}$ is the squared error of the estimator $\\hat{y} = h(x; \\theta)$, otherwise known as the residual sum of squares (RSS), and $SE \\, \\overline{y}$ is the total sum of squares, defined by \n",
    "\n",
    "$$ SE \\, \\overline{y} = \\sum_{i = 1}^{m}(y^{(i)} - \\overline{y})$$ \n",
    "\n",
    "where $\\overline{y}$ is the mean of the $m$ observed $y^{(i)}$'s. \n",
    "\n",
    "Although we are working with squared residuals, notice that we are asking a different question.  Until now, we have used squared error as a performance measure to find the best fitting linear model to our data.  The coefficient of determination is used to evaluate a model selection question, not a model fitting question.  The coefficient of determination measures how much variation is explained by your linear model, ranging between 0 (none) to 1 (all). \n",
    "\n",
    "The method `.score(X,y)` returns the $r^2$ of our model.  Note that we need `X`, which includes the column of intercept terms, rather than merely `x`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sq = model.score(X, y)\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40a2d926733b8622c51557fa9cef47f9",
     "grade": false,
     "grade_id": "cell-fb354467376434b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lastly, the [seaborn](https://seaborn.pydata.org/) statistical visualization library is an extension of matplotlib that includes several built-in features that allow you to create appealing and informative graphics.  For instance, we might like to visualize the <b>confidence interval</b> around our model.  The next two lines of code produce a (default) 95% confidence interval, that is, that interval $[L, U]$ is calculated by\n",
    "\n",
    "$$L = \\theta_1 - 1.96(SE\\,\\theta_1) \\qquad U = \\theta_1 - 1.96(SE\\,\\theta_1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\")\n",
    "sns.regplot(x,y,line_kws = {\"color\": 'r'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8e7f4bbb8e601bcba9fe581618ec6d0",
     "grade": false,
     "grade_id": "cell-e85f30c72c0a163e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# PART C - Your Turn\n",
    "\n",
    "Let's review. In Part A you focused on the mechanics of <b>gradient descent</b>, how the mathematical components of <b>ordinary least squares regression</b> (OLS) regression can be represented and computed by NumPy element-wise array operations, and how those computational components fit together into an implementation of Batch Gradient Descent.  You then ran your implementation of gradient descent to find approximate solutions for the parameter of coefficients, $\\boldsymbol{\\theta}$, and recorded a step-wise history of values of the cost function, showing the progress of the algorithm in finding values of $\\boldsymbol{\\theta}$ taht minimize $J(\\boldsymbol{\\theta})$.  In Part B you used the SciKit-Learn library to confirm the parameter values, were introduced to the \"logic\" of the sklearn API (which you will see more of, later in the course), were introduced to a method -- $r^2$ to evaluate the model, and a brief introduction to the statistical plotting library <b>seaborn</b>\n",
    "\n",
    "In this section, you will do some of the data preparation work that was done for you to fit a regression model on <b>life expectancy</b>.  The data set `ps1_data2.csv` contains the two columns of data in `ps1_data1.csv` plus a third  column recording the average life expectancy, in years, of each country.\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>Note</b>:\n",
    "The average life expectancy data is scaled to values between 0 and 1. For example, a country whose average lifespan is 77.45 years will appear as 0.7745.\n",
    "</div>\n",
    "\n",
    "Your first task is to run the next cell to load `ps1_data2.csv`.  Then, on your own, you will need to prepare the data set to perform a <b>univariate linear regression model</b> to the explain the target variable <i>happiness</i> from the feature </i> average lifespan.  That is, $x$ will be the life expectancy feature, and $y$ will be the happiness score (as before). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('ps1_data2.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "466559189b4f26146c691fe6eb5543e2",
     "grade": false,
     "grade_id": "cell-6451b55b3cb7a74c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once `data2` is in the right format to use the code you have written in Part A and the libraries introduced to you in Part B, use those tools (and those tools only) to answer each part of Question 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A free code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "029e741e71787d6aad720662935827cd",
     "grade": false,
     "grade_id": "cell-310dd6116d27cb85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Question 5\n",
    "\n",
    "Run Gradient descent and sklearn linear regression on `data2`.  Be sure to pick new variables names to ensure that all variables correspond to data set `ps_data2` and not to `ps_data1`. \n",
    "\n",
    "There are four quantities that you will need to report about your OLS univariate regression model explaining happiness scores as a linear function of average life expectancy, which we will refer to as \"model 2\": \n",
    "\n",
    " - a) The value of the intercept parameter $\\theta_0$ of the fitted univariate regression model 2.\n",
    " - b) The value of the coefficient parameter $\\theta_1$ of the fitted univariate regression model 2.\n",
    " - c) The value of the cost function, $J([\\theta_0, \\theta_1])$, on the fitted parameters for model 2.. Note that this is the last value stored in the J_history cost function.\n",
    " - d) The coefficient of determination score of model 2.\n",
    "\n",
    "The test condition for a, b, c, rounds the correct answers to 5 decimal places, using the python function `round(--your answer --, 5)`; the test condition for answer 3 rounds to 3 decimal places. \n",
    "\n",
    "In the following cell, enter the values for for those variables.  Here is the key:\n",
    "\n",
    "~~~python\n",
    "def ans_5():\n",
    "    a =     # a: theta_0 from your fitted model, rounded to 5 decimal places\n",
    "    b =     # b: theta_1 from your fitted model, rounded to 5 decimal places\n",
    "    c =     # c: the last value of the array J_history, rounded to 5 decimal places\n",
    "    d =     # d: r-squared value, rounded to 3 decimal places\n",
    "~~~\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>Hint</b>:\n",
    "If you use tools from Part A and Part B, it is important that they return the same answers (up to 5 decimal places) to those parts of Question 5 that they both can answer. Now the hint: You may want to aim for slightly higher precision for some quantities to ensure the test conditions for all four quantities are satisfied. \n",
    "</div>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afe9335532e6a3c23182dcc011199292",
     "grade": false,
     "grade_id": "ans_five",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ans_five():\n",
    "    \"\"\" Returns four numerical values of your linear model.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    :a:  float\n",
    "        The value of theta_0 from your fitted model, rounded to 5 decimal places.\n",
    "    :b:  float\n",
    "        The value of theta_1 from your fitted model, rounded to 5 decimal places\n",
    "    :c:  float\n",
    "        The last value of the array J_history, rounded to 5 decimal places\n",
    "    :d:  float\n",
    "        The calculated r-squared of your fitted model, rounded to 3 decimal places\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :a:\n",
    "    :b:\n",
    "    :c:\n",
    "    :d:\n",
    "    \"\"\"\n",
    " \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return a, b, c, d  # do not edit\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "899442c373e37bd02a7776efa390d3dc",
     "grade": true,
     "grade_id": "ans_five-test1",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "180fe5c6db8c2b1c82bf6d92c86b91fe",
     "grade": true,
     "grade_id": "ans_five-test2",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64ce946b6297c2d6af0d7249b7bafe89",
     "grade": true,
     "grade_id": "ans_five-test3",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "029dab0f1ec66d4a98e28d6b3b075fa0",
     "grade": true,
     "grade_id": "ans_five-test4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfd9c23759b8e7705c5ceb8b438db650",
     "grade": true,
     "grade_id": "collaborator_policy_test",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test for Collaborator policy before submission\n",
    "%run -i 'collaboration_test.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8e3423eab2d375ab13545f053e6275e",
     "grade": false,
     "grade_id": "cell-3855ebc915820d81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before turning this notebook in, you should do the following steps:\n",
    "  \n",
    "  1. __Restart Kernel__ (Kernel âŸ¶ Restart and Clear Output)\n",
    "  2. __Run all Cells__ (Cell âŸ¶ Run All)\n",
    "  3. __Validate__: Press the 'Validate' button\n",
    "  4. __Save File__ (File âŸ¶ Save and Checkpoint)\n",
    "  5. __Close and Shutdown Kernel__ (File âŸ¶ Close and Halt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
